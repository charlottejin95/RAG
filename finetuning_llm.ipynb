{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charlottejin95/RAG/blob/main/finetuning_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be92eb31",
      "metadata": {
        "id": "be92eb31"
      },
      "source": [
        "# 01-Why_finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "753b9c5b",
      "metadata": {
        "id": "753b9c5b"
      },
      "source": [
        "# Compare finetuned vs. non-finetuned models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d_7UKmTIc61",
      "metadata": {
        "id": "0d_7UKmTIc61"
      },
      "outputs": [],
      "source": [
        "!pip install torch jsonlines pandas datasets transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install safetensors"
      ],
      "metadata": {
        "id": "EQb1CU-tFj-b"
      },
      "id": "EQb1CU-tFj-b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c86d2cd9",
      "metadata": {
        "id": "c86d2cd9"
      },
      "source": [
        "## Try Non-Finetuned models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3d19a8",
      "metadata": {
        "id": "cc3d19a8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef82350",
      "metadata": {
        "id": "1ef82350"
      },
      "outputs": [],
      "source": [
        "model_name = \"openlm-research/open_llama_3b_v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "non_finetuned = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "\n",
        "# non_finetuned.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "213b3936",
      "metadata": {
        "id": "213b3936"
      },
      "outputs": [],
      "source": [
        "input_text = \"Tell me how to train my dog to sit\"\n",
        "non_finetuned_output = non_finetuned.generate(tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device), max_length=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2030a18d",
      "metadata": {
        "id": "2030a18d"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(non_finetuned_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  input_ids = tokenizer.encode(\n",
        "          text,\n",
        "          return_tensors=\"pt\",\n",
        "          truncation=True,\n",
        "          max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "cxJXr86LCJ8q"
      },
      "id": "cxJXr86LCJ8q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd34a82",
      "metadata": {
        "id": "9fd34a82"
      },
      "outputs": [],
      "source": [
        "input_text = \"What do you think of Mars?\"\n",
        "print(inference(input_text, non_finetuned, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22652ae",
      "metadata": {
        "id": "b22652ae"
      },
      "outputs": [],
      "source": [
        "input_text = \"taylor swift's best friend\"\n",
        "print(inference(input_text, non_finetuned, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05d6d640",
      "metadata": {
        "id": "05d6d640"
      },
      "outputs": [],
      "source": [
        "input_text = \"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
        "Customer: I didn't get my item\n",
        "Agent: I'm sorry to hear that. Which item was it?\n",
        "Customer: the blanket\n",
        "Agent:\"\"\"\n",
        "print(inference(input_text, non_finetuned, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del non_finetuned\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "AZ-gMNXBJXF8"
      },
      "id": "AZ-gMNXBJXF8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "809c1d28",
      "metadata": {
        "id": "809c1d28"
      },
      "outputs": [],
      "source": [
        "model_name = \"mediocredev/open-llama-3b-v2-chat\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "finetuned_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "588ad61c",
      "metadata": {
        "id": "588ad61c"
      },
      "outputs": [],
      "source": [
        "input_text = \"Tell me how to train my dog to sit\"\n",
        "finetuned_output = inference(input_text, finetuned_model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d67df1",
      "metadata": {
        "id": "a6d67df1"
      },
      "outputs": [],
      "source": [
        "print(finetuned_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e8f0e7f",
      "metadata": {
        "id": "7e8f0e7f"
      },
      "outputs": [],
      "source": [
        "input_text = \"[INST]Tell me how to train my dog to sit[/INST]\"\n",
        "finetuned_output = inference(input_text, finetuned_model, tokenizer)\n",
        "print(finetuned_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01cf87f6",
      "metadata": {
        "id": "01cf87f6"
      },
      "outputs": [],
      "source": [
        "print(inference(\"[INST]What do you think of Mars?[/INST]\", finetuned_model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32db06c7",
      "metadata": {
        "id": "32db06c7"
      },
      "outputs": [],
      "source": [
        "# print(finetuned_model(\"taylor swift's best friend\"))\n",
        "print(inference(\"[INST]taylor swift's best friend[/INST]\", finetuned_model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be48bda1",
      "metadata": {
        "id": "be48bda1"
      },
      "outputs": [],
      "source": [
        "input_text = \"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
        "Customer: I didn't get my item\n",
        "Agent: I'm sorry to hear that. Which item was it?\n",
        "Customer: the blanket\n",
        "Agent:\"\"\"\n",
        "print(inference(input_text, finetuned_model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"[INST]Agent: I'm here to help you with your Amazon deliver order.\n",
        "Customer: I didn't get my item\n",
        "Agent: I'm sorry to hear that. Which item was it?\n",
        "Customer: the blanket\n",
        "Agent:???[/INST]\"\"\"\n",
        "print(inference(input_text, finetuned_model, tokenizer))"
      ],
      "metadata": {
        "id": "JrLPI7vWa3ri"
      },
      "id": "JrLPI7vWa3ri",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del finetuned_model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "q54geZuHM4yd"
      },
      "id": "q54geZuHM4yd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9f512b13",
      "metadata": {
        "id": "9f512b13"
      },
      "source": [
        "# 02-Where_finetuning_fits_in"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b944c304",
      "metadata": {
        "id": "b944c304"
      },
      "source": [
        "# Finetuning data: compare to pretraining and basic preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec5e3b3",
      "metadata": {
        "id": "fec5e3b3"
      },
      "outputs": [],
      "source": [
        "import jsonlines #把每条数据变成一行\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from pprint import pprint #输出\n",
        "\n",
        "import datasets #根据dataset的名字就可以load进来data\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cd7700c",
      "metadata": {
        "id": "3cd7700c"
      },
      "source": [
        "## Look at pretraining data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc92a955",
      "metadata": {
        "id": "bc92a955"
      },
      "outputs": [],
      "source": [
        "#pretrained_dataset = load_dataset(\"EleutherAI/pile\", split=\"train\", streaming=True)\n",
        "\n",
        "pretrained_dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n",
        "#把数据load进来，c4是dataset的名字\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae5f773",
      "metadata": {
        "id": "5ae5f773"
      },
      "outputs": [],
      "source": [
        "n = 5\n",
        "print(\"Pretrained dataset:\")\n",
        "top_n = itertools.islice(pretrained_dataset, n)\n",
        "for i in top_n:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc33338d",
      "metadata": {
        "id": "cc33338d"
      },
      "source": [
        "## Contrast with company finetuning dataset you will be using"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66668b38",
      "metadata": {
        "id": "66668b38"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "dataset = load_dataset('lamini/lamini_docs')\n",
        "\n",
        "# Display the dataset\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the 'train' split\n",
        "train_dataset = dataset['train']\n",
        "\n",
        "# Display the first few examples\n",
        "for i in range(5):  # Adjust the range as needed\n",
        "    print(train_dataset[i])\n"
      ],
      "metadata": {
        "id": "JOyAQ2gdPR1x"
      },
      "id": "JOyAQ2gdPR1x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b84160b8",
      "metadata": {
        "id": "b84160b8"
      },
      "source": [
        "## Various ways of formatting your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca6b7e7",
      "metadata": {
        "id": "dca6b7e7"
      },
      "outputs": [],
      "source": [
        "examples = train_dataset\n",
        "text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "#把question和answer连起来\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b0fae24",
      "metadata": {
        "id": "5b0fae24"
      },
      "outputs": [],
      "source": [
        "if \"question\" in examples and \"answer\" in examples:\n",
        "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "elif \"instruction\" in examples and \"response\" in examples:\n",
        "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
        "elif \"input\" in examples and \"output\" in examples:\n",
        "  text = examples[\"input\"][0] + examples[\"output\"][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc40e0ab",
      "metadata": {
        "id": "cc40e0ab"
      },
      "outputs": [],
      "source": [
        "prompt_template_qa = \"\"\"### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\n",
        "{answer}\"\"\"\n",
        "\n",
        "#三个井号，类似于INST标符，提示哪些是question，哪些是answer，可以更好提示model\n",
        "#在使用chatGPT时候也可以有更好效果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "055b5661",
      "metadata": {
        "id": "055b5661"
      },
      "outputs": [],
      "source": [
        "question = examples[\"question\"][0]\n",
        "answer = examples[\"answer\"][0]\n",
        "\n",
        "text_with_prompt_template = prompt_template_qa.format(question=question, answer=answer)\n",
        "text_with_prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b1923b",
      "metadata": {
        "id": "11b1923b"
      },
      "outputs": [],
      "source": [
        "prompt_template_q = \"\"\"### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f81a6078",
      "metadata": {
        "id": "f81a6078"
      },
      "outputs": [],
      "source": [
        "num_examples = len(examples[\"question\"])\n",
        "finetuning_dataset_text_only = []\n",
        "finetuning_dataset_question_answer = []\n",
        "for i in range(num_examples):\n",
        "  question = examples[\"question\"][i]\n",
        "  answer = examples[\"answer\"][i]\n",
        "\n",
        "  text_with_prompt_template_qa = prompt_template_qa.format(question=question, answer=answer)\n",
        "  finetuning_dataset_text_only.append({\"text\": text_with_prompt_template_qa})\n",
        "\n",
        "  text_with_prompt_template_q = prompt_template_q.format(question=question)\n",
        "  finetuning_dataset_question_answer.append({\"question\": text_with_prompt_template_q, \"answer\": answer})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f517adf4",
      "metadata": {
        "id": "f517adf4"
      },
      "outputs": [],
      "source": [
        "pprint(finetuning_dataset_text_only[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5febb8c8",
      "metadata": {
        "id": "5febb8c8"
      },
      "outputs": [],
      "source": [
        "pprint(finetuning_dataset_question_answer[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e93258d",
      "metadata": {
        "id": "7e93258d"
      },
      "source": [
        "## Common ways of storing your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38e91b0a",
      "metadata": {
        "id": "38e91b0a"
      },
      "outputs": [],
      "source": [
        "#保存成JSON，节省空间，就是生成了一个文本\n",
        "with jsonlines.open(f'lamini_docs_processed.jsonl', 'w') as writer:\n",
        "    writer.write_all(finetuning_dataset_question_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "517cfc56",
      "metadata": {
        "id": "517cfc56"
      },
      "outputs": [],
      "source": [
        "finetuning_dataset_name = \"lamini/lamini_docs\"\n",
        "finetuning_dataset = load_dataset(finetuning_dataset_name)\n",
        "print(finetuning_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "501de373",
      "metadata": {
        "id": "501de373"
      },
      "source": [
        "# 03-Instruction-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5729f6ce",
      "metadata": {
        "id": "5729f6ce"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import jsonlines\n",
        "\n",
        "from datasets import load_dataset\n",
        "from pprint import pprint\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a712487",
      "metadata": {
        "id": "4a712487"
      },
      "source": [
        "## Load instruction tuned dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a4ed11a",
      "metadata": {
        "id": "1a4ed11a"
      },
      "outputs": [],
      "source": [
        "instruction_tuned_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", streaming=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e48415c",
      "metadata": {
        "id": "9e48415c"
      },
      "outputs": [],
      "source": [
        "m = 5\n",
        "print(\"Instruction-tuned dataset:\")\n",
        "top_m = list(itertools.islice(instruction_tuned_dataset, m))\n",
        "for j in top_m:\n",
        "  print(j)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54f2cf23",
      "metadata": {
        "id": "54f2cf23"
      },
      "source": [
        "## Two prompt templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "441aeced",
      "metadata": {
        "id": "441aeced"
      },
      "outputs": [],
      "source": [
        "prompt_template_with_input = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "#在finetuning的时候给出很明确的instruction，规则和限制，来更好完成任务；\n",
        "#和之前的template区别主要就是给了instruction，给了更明确的东西\n",
        "\n",
        "prompt_template_without_input = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26a977e7",
      "metadata": {
        "id": "26a977e7"
      },
      "source": [
        "## Hydrate prompts (add data to prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70506882",
      "metadata": {
        "id": "70506882"
      },
      "outputs": [],
      "source": [
        "processed_data = []\n",
        "for j in top_m:\n",
        "  if not j[\"input\"]:\n",
        "    processed_prompt = prompt_template_without_input.format(instruction=j[\"instruction\"])\n",
        "  else:\n",
        "    processed_prompt = prompt_template_with_input.format(instruction=j[\"instruction\"], input=j[\"input\"])\n",
        "\n",
        "  processed_data.append({\"input\": processed_prompt, \"output\": j[\"output\"]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f176a0b",
      "metadata": {
        "id": "3f176a0b"
      },
      "outputs": [],
      "source": [
        "pprint(processed_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ad87987",
      "metadata": {
        "id": "6ad87987"
      },
      "source": [
        "## Save data to jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8186af6",
      "metadata": {
        "id": "d8186af6"
      },
      "outputs": [],
      "source": [
        "with jsonlines.open(f'alpaca_processed.jsonl', 'w') as writer:\n",
        "    writer.write_all(processed_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "277b4ad5",
      "metadata": {
        "id": "277b4ad5"
      },
      "source": [
        "## Try smaller models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c37ef86e",
      "metadata": {
        "id": "c37ef86e"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afe5ede0",
      "metadata": {
        "id": "afe5ede0"
      },
      "outputs": [],
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize 定义怎么生成token\n",
        "  input_ids = tokenizer.encode(\n",
        "          text,\n",
        "          return_tensors=\"pt\",\n",
        "          truncation=True,\n",
        "          max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f5e568",
      "metadata": {
        "id": "22f5e568"
      },
      "outputs": [],
      "source": [
        "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
        "finetuning_dataset = load_dataset(finetuning_dataset_path)\n",
        "print(finetuning_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b77abca6",
      "metadata": {
        "id": "b77abca6"
      },
      "outputs": [],
      "source": [
        "test_sample = finetuning_dataset[\"test\"][0]\n",
        "print(test_sample)\n",
        "\n",
        "print(inference(test_sample[\"question\"], model, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bfe428d",
      "metadata": {
        "id": "8bfe428d"
      },
      "source": [
        "## Compare to finetuned small model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3df65ed",
      "metadata": {
        "id": "e3df65ed"
      },
      "outputs": [],
      "source": [
        "instruction_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e4c39c3",
      "metadata": {
        "id": "9e4c39c3"
      },
      "outputs": [],
      "source": [
        "print(inference(test_sample[\"question\"], instruction_model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da151c2",
      "metadata": {
        "id": "3da151c2"
      },
      "outputs": [],
      "source": [
        "# Pssst! If you were curious how to upload your own dataset to Huggingface\n",
        "# Here is how we did it\n",
        "\n",
        "# !pip install huggingface_hub\n",
        "# !huggingface-cli login\n",
        "\n",
        "# import pandas as pd\n",
        "# import datasets\n",
        "# from datasets import Dataset\n",
        "\n",
        "# finetuning_dataset = Dataset.from_pandas(pd.DataFrame(data=finetuning_dataset))\n",
        "# finetuning_dataset.push_to_hub(dataset_path_hf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c55279f",
      "metadata": {
        "id": "1c55279f"
      },
      "source": [
        "# 04-Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfc9f672",
      "metadata": {
        "id": "bfc9f672"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import datasets\n",
        "\n",
        "from pprint import pprint\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed223760",
      "metadata": {
        "id": "ed223760"
      },
      "source": [
        "## Tokenizing text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f247cd15",
      "metadata": {
        "id": "f247cd15"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f55e08",
      "metadata": {
        "id": "37f55e08"
      },
      "outputs": [],
      "source": [
        "text = \"Hi, how are you?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f143c0f3",
      "metadata": {
        "id": "f143c0f3"
      },
      "outputs": [],
      "source": [
        "encoded_text = tokenizer(text)[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec714c33",
      "metadata": {
        "id": "ec714c33"
      },
      "outputs": [],
      "source": [
        "encoded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f31a63",
      "metadata": {
        "id": "50f31a63"
      },
      "outputs": [],
      "source": [
        "decoded_text = tokenizer.decode(encoded_text)\n",
        "print(\"Decoded tokens back into text: \", decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa452dec",
      "metadata": {
        "id": "aa452dec"
      },
      "source": [
        "## Tokenize multiple texts at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04882f2a",
      "metadata": {
        "id": "04882f2a"
      },
      "outputs": [],
      "source": [
        "list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n",
        "encoded_texts = tokenizer(list_texts)\n",
        "print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf2e59a4",
      "metadata": {
        "id": "bf2e59a4"
      },
      "source": [
        "## Padding and truncation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09fdbb16",
      "metadata": {
        "id": "09fdbb16"
      },
      "outputs": [],
      "source": [
        "#把不同长度字符拼在一起，padding填没意义的字符，让字符对其，这样才能并行计算\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
        "print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c210247b",
      "metadata": {
        "id": "c210247b"
      },
      "outputs": [],
      "source": [
        "encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
        "print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41eaf7ba",
      "metadata": {
        "id": "41eaf7ba"
      },
      "outputs": [],
      "source": [
        "tokenizer.truncation_side = \"left\"\n",
        "encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
        "print(\"Using left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c5d71e",
      "metadata": {
        "id": "80c5d71e"
      },
      "outputs": [],
      "source": [
        "encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
        "print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00d09d35",
      "metadata": {
        "id": "00d09d35"
      },
      "source": [
        "## Prepare instruction dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b18547",
      "metadata": {
        "id": "42b18547"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = 'lamini/lamini_docs'\n",
        "dataset = load_dataset(filename)\n",
        "examples = dataset['train']\n",
        "\n",
        "if \"question\" in examples and \"answer\" in examples:\n",
        "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "elif \"instruction\" in examples and \"response\" in examples:\n",
        "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
        "elif \"input\" in examples and \"output\" in examples:\n",
        "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
        "\n",
        "#进行一些数据拼接工作\n",
        "prompt_template = \"\"\"### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\"\"\"\n",
        "\n",
        "num_examples = len(examples[\"question\"])\n",
        "finetuning_dataset = []\n",
        "for i in range(num_examples):\n",
        "  question = examples[\"question\"][i]\n",
        "  answer = examples[\"answer\"][i]\n",
        "  text_with_prompt_template = prompt_template.format(question=question)\n",
        "  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
        "\n",
        "from pprint import pprint\n",
        "print(\"One datapoint in the finetuning dataset:\")\n",
        "pprint(finetuning_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ab0789b",
      "metadata": {
        "id": "6ab0789b"
      },
      "source": [
        "## Tokenize a single example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c320ab",
      "metadata": {
        "id": "b4c320ab"
      },
      "outputs": [],
      "source": [
        "text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
        "tokenized_inputs = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"np\",\n",
        "    padding=True\n",
        ")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1674bc66",
      "metadata": {
        "id": "1674bc66"
      },
      "outputs": [],
      "source": [
        "max_length = 2048\n",
        "max_length = min(\n",
        "    tokenized_inputs[\"input_ids\"].shape[1],\n",
        "    max_length,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c1db47b",
      "metadata": {
        "id": "4c1db47b"
      },
      "outputs": [],
      "source": [
        "tokenized_inputs = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"np\",\n",
        "    truncation=True,\n",
        "    max_length=max_length\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fdaaa14",
      "metadata": {
        "id": "0fdaaa14"
      },
      "outputs": [],
      "source": [
        "tokenized_inputs[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad4c8c54",
      "metadata": {
        "id": "ad4c8c54"
      },
      "source": [
        "## Tokenize the instruction dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b8d596b",
      "metadata": {
        "id": "9b8d596b"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    if \"question\" in examples and \"answer\" in examples:\n",
        "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "    elif \"input\" in examples and \"output\" in examples:\n",
        "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
        "    else:\n",
        "      text = examples[\"output\"][0]\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        padding=True,\n",
        "    )\n",
        "\n",
        "    max_length = min(\n",
        "        tokenized_inputs[\"input_ids\"].shape[1],\n",
        "        2048\n",
        "    )\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef040ff",
      "metadata": {
        "id": "4ef040ff"
      },
      "outputs": [],
      "source": [
        "finetuning_dataset_loaded = datasets.load_dataset(filename, split=\"train\")\n",
        "\n",
        "tokenized_dataset = finetuning_dataset_loaded.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=1,\n",
        "    drop_last_batch=True\n",
        ")\n",
        "\n",
        "print(tokenized_dataset)\n",
        "pprint(tokenized_dataset[0])\n",
        "\n",
        "#把文本编程数字的形式"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4918320f",
      "metadata": {
        "id": "4918320f"
      },
      "source": [
        "## Prepare test/train splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa77e06e",
      "metadata": {
        "id": "aa77e06e"
      },
      "outputs": [],
      "source": [
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
        "print(split_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c73ea8f8",
      "metadata": {
        "id": "c73ea8f8"
      },
      "source": [
        "### Some datasets for you to try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3c9fea2",
      "metadata": {
        "id": "a3c9fea2"
      },
      "outputs": [],
      "source": [
        "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
        "finetuning_dataset = datasets.load_dataset(finetuning_dataset_path)\n",
        "print(finetuning_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc5cb28e",
      "metadata": {
        "id": "cc5cb28e"
      },
      "outputs": [],
      "source": [
        "taylor_swift_dataset = \"lamini/taylor_swift\"\n",
        "bts_dataset = \"lamini/bts\"\n",
        "open_llms = \"lamini/open_llms\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f28cef5d",
      "metadata": {
        "id": "f28cef5d"
      },
      "outputs": [],
      "source": [
        "dataset_swiftie = datasets.load_dataset(taylor_swift_dataset)\n",
        "print(dataset_swiftie[\"train\"][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26f7c645",
      "metadata": {
        "id": "26f7c645"
      },
      "source": [
        "# 05-Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fdf34b7",
      "metadata": {
        "id": "3fdf34b7"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import tempfile\n",
        "import logging\n",
        "import random\n",
        "# import config\n",
        "import os\n",
        "import yaml\n",
        "import time\n",
        "import torch\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import jsonlines\n",
        "\n",
        "# from utilities import *\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "global_config = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84afec4f",
      "metadata": {
        "id": "84afec4f"
      },
      "source": [
        "## Load the Lamini docs dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a50c1f96",
      "metadata": {
        "id": "a50c1f96"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"lamini/lamini_docs\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66bdd766",
      "metadata": {
        "id": "66bdd766"
      },
      "source": [
        "## Set up the model, training config, and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d5204b4",
      "metadata": {
        "id": "1d5204b4"
      },
      "outputs": [],
      "source": [
        "model_name = \"EleutherAI/pythia-70m\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81716952",
      "metadata": {
        "id": "81716952"
      },
      "outputs": [],
      "source": [
        "# training_config = {\n",
        "#     \"model\": {\n",
        "#         \"pretrained_name\": model_name,\n",
        "#         \"max_length\" : 2048\n",
        "#     },\n",
        "#     \"datasets\": {\n",
        "#         \"use_hf\": use_hf,\n",
        "#         \"path\": dataset_path\n",
        "#     },\n",
        "#     \"verbose\": True\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b113545",
      "metadata": {
        "id": "6b113545"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "dataset = load_dataset(dataset_path)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "print(train_dataset)\n",
        "print(test_dataset)\n",
        "\n",
        "# train_dataset = train_dataset.map(\n",
        "#     tokenize_function,\n",
        "#     batched=True,\n",
        "#     batch_size=1,\n",
        "#     drop_last_batch=True\n",
        "# )\n",
        "# test_dataset = test_dataset.map(\n",
        "#     tokenize_function,\n",
        "#     batched=True,\n",
        "#     batch_size=1,\n",
        "#     drop_last_batch=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20661c62",
      "metadata": {
        "id": "20661c62"
      },
      "source": [
        "## Load the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ad86481",
      "metadata": {
        "id": "1ad86481"
      },
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f470887",
      "metadata": {
        "id": "3f470887"
      },
      "outputs": [],
      "source": [
        "device_count = torch.cuda.device_count()\n",
        "if device_count > 0:\n",
        "    logger.debug(\"Select GPU device\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    logger.debug(\"Select CPU device\")\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afcccc87",
      "metadata": {
        "id": "afcccc87"
      },
      "outputs": [],
      "source": [
        "base_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6bebeec",
      "metadata": {
        "id": "e6bebeec"
      },
      "source": [
        "## Define function to carry out inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46d461b8",
      "metadata": {
        "id": "46d461b8"
      },
      "outputs": [],
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  input_ids = tokenizer.encode(\n",
        "          text,\n",
        "          return_tensors=\"pt\",\n",
        "          truncation=True,\n",
        "          max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a7pJkRFxuqFJ"
      },
      "id": "a7pJkRFxuqFJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4c312738",
      "metadata": {
        "id": "4c312738"
      },
      "source": [
        "## Try the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9f4307",
      "metadata": {
        "id": "5c9f4307"
      },
      "outputs": [],
      "source": [
        "test_text = test_dataset[0]['question']\n",
        "print(\"Question input (test):\", test_text)\n",
        "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
        "print(\"Model's answer: \")\n",
        "print(inference(test_text, base_model, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32bc6275",
      "metadata": {
        "id": "32bc6275"
      },
      "source": [
        "### Setup training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecaf5f7e",
      "metadata": {
        "id": "ecaf5f7e"
      },
      "outputs": [],
      "source": [
        "max_steps = 240"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402887be",
      "metadata": {
        "id": "402887be"
      },
      "outputs": [],
      "source": [
        "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
        "output_dir = trained_model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ef1ff0",
      "metadata": {
        "id": "02ef1ff0"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "\n",
        "  # Learning rate\n",
        "  learning_rate=1.0e-5,\n",
        "\n",
        "  # Number of training epochs\n",
        "  num_train_epochs=1,\n",
        "\n",
        "  # Max steps to train for (each step is a batch of data)\n",
        "  # Overrides num_train_epochs, if not -1\n",
        "  max_steps=max_steps,\n",
        "\n",
        "  # Batch size for training\n",
        "  per_device_train_batch_size=1,\n",
        "\n",
        "  # Directory to save model checkpoints\n",
        "  output_dir=output_dir,\n",
        "\n",
        "  # Other arguments\n",
        "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
        "  disable_tqdm=False, # Disable progress bars\n",
        "  eval_steps=10, # Number of update steps between two evaluations\n",
        "  save_steps=120, # After # steps model is saved\n",
        "  warmup_steps=0, # Number of warmup steps for learning rate scheduler\n",
        "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
        "  evaluation_strategy=\"steps\",\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=1,\n",
        "  optim=\"adafactor\",\n",
        "  gradient_accumulation_steps = 4,\n",
        "  gradient_checkpointing=False,\n",
        "\n",
        "  # Parameters for early stopping\n",
        "  load_best_model_at_end=True,\n",
        "  save_total_limit=1,\n",
        "  metric_for_best_model=\"eval_loss\",\n",
        "  greater_is_better=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea9d96f4",
      "metadata": {
        "id": "ea9d96f4"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=base_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1de4bd9",
      "metadata": {
        "id": "f1de4bd9"
      },
      "source": [
        "### Train a few steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab6f5ff",
      "metadata": {
        "id": "6ab6f5ff"
      },
      "outputs": [],
      "source": [
        "training_output = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a85b793e",
      "metadata": {
        "id": "a85b793e"
      },
      "source": [
        "### Save model locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27a79b1b",
      "metadata": {
        "id": "27a79b1b"
      },
      "outputs": [],
      "source": [
        "save_dir = f'{output_dir}/final'\n",
        "\n",
        "trainer.save_model(save_dir)\n",
        "print(\"Saved model to:\", save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "915cefdd",
      "metadata": {
        "id": "915cefdd"
      },
      "outputs": [],
      "source": [
        "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3de1166",
      "metadata": {
        "id": "c3de1166"
      },
      "outputs": [],
      "source": [
        "finetuned_slightly_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77850200",
      "metadata": {
        "id": "77850200"
      },
      "source": [
        "### Run slightly trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83821430",
      "metadata": {
        "id": "83821430"
      },
      "outputs": [],
      "source": [
        "test_question = test_dataset[0]['question']\n",
        "print(\"Question input (test):\", test_question)\n",
        "\n",
        "print(\"Finetuned slightly model's answer: \")\n",
        "print(inference(test_question, finetuned_slightly_model, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1569683a",
      "metadata": {
        "id": "1569683a"
      },
      "outputs": [],
      "source": [
        "test_answer = test_dataset[0]['answer']\n",
        "print(\"Target answer output (test):\", test_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1d52885",
      "metadata": {
        "id": "c1d52885"
      },
      "source": [
        "### Run same model trained for two epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ef0552",
      "metadata": {
        "id": "b5ef0552"
      },
      "outputs": [],
      "source": [
        "finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
        "\n",
        "finetuned_longer_model.to(device)\n",
        "print(\"Finetuned longer model's answer: \")\n",
        "print(inference(test_question, finetuned_longer_model, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96bb3285",
      "metadata": {
        "id": "96bb3285"
      },
      "source": [
        "### Run much larger trained model and explore moderation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8a33d49",
      "metadata": {
        "id": "e8a33d49"
      },
      "source": [
        "# Explore moderation using small model\n",
        "### First, try the non-finetuned base model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd3bebef",
      "metadata": {
        "id": "dd3bebef"
      },
      "outputs": [],
      "source": [
        "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b62ad60",
      "metadata": {
        "id": "4b62ad60"
      },
      "source": [
        "### Now try moderation with finetuned small model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1d74b1",
      "metadata": {
        "id": "1e1d74b1"
      },
      "outputs": [],
      "source": [
        "print(inference(\"What do you think of Mars?\", finetuned_longer_model, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa404373",
      "metadata": {
        "id": "fa404373"
      },
      "source": [
        "# 06-Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "782fa1ff",
      "metadata": {
        "id": "782fa1ff"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import tempfile\n",
        "import logging\n",
        "import random\n",
        "import os\n",
        "import yaml\n",
        "import logging\n",
        "import difflib\n",
        "import pandas as pd\n",
        "\n",
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "global_config = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8151522c",
      "metadata": {
        "id": "8151522c"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.load_dataset(\"lamini/lamini_docs\")\n",
        "\n",
        "test_dataset = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b3fb9e2",
      "metadata": {
        "id": "8b3fb9e2"
      },
      "outputs": [],
      "source": [
        "print(test_dataset[0][\"question\"])\n",
        "print(test_dataset[0][\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e9ef08d",
      "metadata": {
        "id": "8e9ef08d"
      },
      "outputs": [],
      "source": [
        "model_name = \"lamini/lamini_docs_finetuned\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0101f82b",
      "metadata": {
        "id": "0101f82b"
      },
      "source": [
        "## Setup a really basic evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3a592a",
      "metadata": {
        "id": "5f3a592a"
      },
      "outputs": [],
      "source": [
        "def is_exact_match(a, b):\n",
        "    return a.strip() == b.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c04747b8",
      "metadata": {
        "id": "c04747b8"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb95acbb",
      "metadata": {
        "id": "fb95acbb"
      },
      "outputs": [],
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  input_ids = tokenizer.encode(\n",
        "      text,\n",
        "      return_tensors=\"pt\",\n",
        "      truncation=True,\n",
        "      max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a06fb78b",
      "metadata": {
        "id": "a06fb78b"
      },
      "source": [
        "## Run model and compare to expected answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a5a213",
      "metadata": {
        "id": "53a5a213"
      },
      "outputs": [],
      "source": [
        "test_question = test_dataset[0][\"question\"]\n",
        "generated_answer = inference(test_question, model, tokenizer)\n",
        "print(test_question)\n",
        "print(generated_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c5aba95",
      "metadata": {
        "id": "8c5aba95"
      },
      "outputs": [],
      "source": [
        "answer = test_dataset[0][\"answer\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71f4366b",
      "metadata": {
        "id": "71f4366b"
      },
      "outputs": [],
      "source": [
        "exact_match = is_exact_match(generated_answer, answer)\n",
        "print(exact_match)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a623146",
      "metadata": {
        "id": "7a623146"
      },
      "source": [
        "## Run over entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e4b91ed",
      "metadata": {
        "id": "4e4b91ed"
      },
      "outputs": [],
      "source": [
        "n = 10\n",
        "metrics = {'exact_matches': []}\n",
        "predictions = []\n",
        "for i, item in tqdm(enumerate(test_dataset)):\n",
        "    print(\"i Evaluating: \" + str(item))\n",
        "    question = item['question']\n",
        "    answer = item['answer']\n",
        "\n",
        "    try:\n",
        "      predicted_answer = inference(question, model, tokenizer)\n",
        "    except:\n",
        "      continue\n",
        "    predictions.append([predicted_answer, answer])\n",
        "\n",
        "    #fixed: exact_match = is_exact_match(generated_answer, answer)\n",
        "    exact_match = is_exact_match(predicted_answer, answer)\n",
        "    metrics['exact_matches'].append(exact_match)\n",
        "\n",
        "    if i > n and n != -1:\n",
        "      break\n",
        "print('Number of exact matches: ', sum(metrics['exact_matches']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7fa23f5",
      "metadata": {
        "id": "a7fa23f5"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(predictions, columns=[\"predicted_answer\", \"target_answer\"])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0141bd04",
      "metadata": {
        "id": "0141bd04"
      },
      "source": [
        "## Evaluate all the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd0ac19",
      "metadata": {
        "id": "2cd0ac19"
      },
      "outputs": [],
      "source": [
        "evaluation_dataset_path = \"lamini/lamini_docs_evaluation\"\n",
        "evaluation_dataset = datasets.load_dataset(evaluation_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2fb53d1",
      "metadata": {
        "id": "a2fb53d1"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(evaluation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 07-Deeper into Transformer"
      ],
      "metadata": {
        "id": "fcOnGFldyU7F"
      },
      "id": "fcOnGFldyU7F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check about the shape of input and output"
      ],
      "metadata": {
        "id": "o_Sp45Tyynrl"
      },
      "id": "o_Sp45Tyynrl"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "text = 'GPT, short for Generative Pre-trained Transformer, represents a groundbreaking advancement in the field of artificial intelligence and natural language processing. Developed by OpenAI, GPT is designed to understand, generate, and interpret human language with remarkable accuracy and fluency. It operates on the principle of machine learning, where the model is initially pre-trained on a vast corpus of text data. This pre-training enables GPT to grasp the intricacies of language, including grammar, context, and even subtleties like humor and sarcasm. Following the pre-training phase, GPT undergoes fine-tuning, where it is further trained on a smaller, more specialized dataset to perform specific tasks like translation, question-answering, and content creation. What sets GPT apart is its deep learning architecture, which consists of multiple layers of transformers—hence the name. These transformers allow the model to process and analyze text in a highly efficient and nuanced manner, making GPT capable of generating text that is often indistinguishable from that written by humans. As technology evolves, GPT continues to push the boundaries of what artificial intelligence can achieve in understanding and mimicking human language.'\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"Characters from the sentence:\", \"\".join(chars))\n",
        "print(\"vocab_size from the sentence: \", vocab_size)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "train_data = data\n",
        "\n",
        "def get_batch():\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = len(data) - 1 # what is the maximum context length for predictions?\n",
        "# block_size = 192 # what is the maximum context length for predictions?\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "n_embd = 1500\n",
        "n_head = 1\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()   # head_size = 150\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)   # x  ->  embedding size ->  n_head * head_size\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):  # head_size = 150 * n_head 10\n",
        "        B,T,C = x.shape  # batch_size, seq_len, embedding_size   (4, 100, 1500)\n",
        "        k = self.key(x)   # (B,T,C)  (4, 100, 1500) * (1500, 150) -> (4, 100, 150)\n",
        "        q = self.query(x) # (B,T,C)  (4, 100, 1500) * (1500, 150) -> (4, 100, 150)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)  （4, 100, 150) * (4, 150, 100) -> (4, 100, 100)\n",
        "             # you are the best\n",
        "        # you  11  21   23  23\n",
        "        # are\n",
        "        # the\n",
        "        # best\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)  (4, 100, 100)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)  (4, 100, 100) * (100, 1500) -> (4, 100, 150)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)  # n_head * head_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)      # [1],[2],[3]  -> [1,2,3]  # (4, 100, 150) * 10 -> (4, 100, 1500) -> batch, seq_length, embedding_size\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),   # 64 -> 256   信息 -> 维度升高\n",
        "            nn.ReLU(),                  # 激活函数      取出强烈的信息\n",
        "            nn.Linear(4 * n_embd, n_embd),   # 信息维度降低\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)  # 提取信息\n",
        "        self.ffwd = FeedFoward(n_embd)  # GPT感知信息\n",
        "        self.ln1 = nn.LayerNorm(n_embd)  # 归一化\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BabyGPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)  # 词嵌入  [0,2,5,6] -> [0.1,0.2,0,7,0.1]  n_embed\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)   #                  [0.9,0.3,0.4,0.1]\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])   # transformer blocks * 4\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)   # 词嵌入 -> 线性层(形状变换) -> logits []  \"your name is GPT-3\" -> seq_len * [0.9,0.3,0.4,0.1] -> \"信息 -> 概率\"  -> token的概率 生成某个token的概率   [0.1, 0.1, 0.8, 0.0]\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers m   idx + next_idx = [y], [yo],[you]\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)   batch_size * seq_len * [0.9,0.3,0.4,0.1]\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)   (T,C)  (4, 100, 1500)\n",
        "        x = tok_emb + pos_emb # (B,T,C)  # 词信息 + 位置信息 (4, 100, 1500)\n",
        "        x = self.blocks(x) # (B,T,C)  # 信息提取 (4, 100, 1500) -> 10 * (4, 100, 150) -> (4, 100, 1500)\n",
        "        x = self.ln_f(x) # (B,T,C)  # 归一化\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)  # 词嵌入 -> 线性层(形状变换) -> logits(概率信息) (4, 100, 1500) * (1500, 39) -> (4, 100, 39)\n",
        "         # I am st .. o\n",
        "         # [ ,'a', .... , ]\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)   # 交叉熵损失函数  - 差距  - minimize 差距\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):   # data pre -> train model -> model serving\n",
        "        # idx is (B, T) array of indices in the current context  # \"pre\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond) # [0,3,2,4,5,6]\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C) # [0.1, 0.2, 0.7]  (4, 100, 39) -> (4, 39)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)  # [0.1, 0.44, 0.46]  -> max 2  boss\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)   \"pre\"  -  I am a  -> \"I am a \" + \"boss\" -> \"I am a boss\" -> model -> \"I am a boss\" -> \"I am a boss\" + \"!\" s\n",
        "        return idx"
      ],
      "metadata": {
        "id": "U31R772PyfUS"
      },
      "id": "U31R772PyfUS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test with a babyGPT"
      ],
      "metadata": {
        "id": "G_oaLGsty4Bf"
      },
      "id": "G_oaLGsty4Bf"
    },
    {
      "cell_type": "code",
      "source": [
        "m = BabyGPT()"
      ],
      "metadata": {
        "id": "OMQ_yzoey78G"
      },
      "id": "OMQ_yzoey78G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 1\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "m = m.to(device)\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "# 使用tqdm添加进度条，并在进度条中显示中间loss值\n",
        "pbar = tqdm(range(500))\n",
        "for steps in pbar:\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch()\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 更新进度条的描述以显示当前的loss值\n",
        "    pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "TNAcjKfDy-iR"
      },
      "id": "TNAcjKfDy-iR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_id = torch.tensor([encode('GPT')], dtype=torch.long)\n",
        "start_id = start_id.to(device)\n",
        "print(decode(m.generate(idx = start_id, max_new_tokens=200)[0].tolist()))"
      ],
      "metadata": {
        "id": "Vqr6cfpFzMZS"
      },
      "id": "Vqr6cfpFzMZS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rethink about the attention mechanism"
      ],
      "metadata": {
        "id": "41xSiR86zLut"
      },
      "id": "41xSiR86zLut"
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "\n",
        "\n",
        "# you are the best\n",
        "# 1    2    3   4\n",
        "torch.manual_seed(42)\n",
        "wei = torch.tril(torch.ones(7, 7))\n",
        "wei = wei / torch.sum(wei, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(7,2)).float()\n",
        "c = wei @ b\n",
        "print('wei=')\n",
        "print(wei)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)\n",
        "\n",
        "     # you are the best\n",
        "\n",
        "# you  11  00   00  00\n",
        "# are  14  23   00  00\n",
        "# the\n",
        "# best"
      ],
      "metadata": {
        "id": "MmTXQGKSzq91"
      },
      "id": "MmTXQGKSzq91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self-attention!\n",
        "#import torch.nn as nn\n",
        "#from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 1,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)  # (1, 8, 32)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)  # x -->捕捉可以当作key的信息\n",
        "query = nn.Linear(C, head_size, bias=False) # x -->捕捉可以当作query的信息\n",
        "value = nn.Linear(C, head_size, bias=False) # x -->捕捉可以当作value的信息\n",
        "\n",
        "print(key.weight)\n",
        "print(query)\n",
        "print(value)\n",
        "\n",
        "k = key(x)   # (B, T, 16)  (1, 8, 32) * (32, 16)  (1, 8, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)  attention score   (1, 8, 8)\n",
        "\n",
        "print(wei[0])\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(\"masked:\",wei[0])\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(\"masked:\",wei[0])\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "sg81wSQ70t_p"
      },
      "id": "sg81wSQ70t_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "id": "907hYo_l0v-j"
      },
      "id": "907hYo_l0v-j",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}